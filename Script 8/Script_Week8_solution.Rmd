---
title: "Script_Week 8: Practice session II"
author: "Ana-Maria Plesca"
date: "2023-05-12"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, echo=F, message=FALSE, warning=FALSE}
# Loading the necessary packages
library(tidyverse)
```

This is the second practice session in R. Please work individually or in groups and make sure to hand out the script by 22.06.2023, 23:59. If you decide to work in groups to complete this practice session, please hand in your own, error-free knittable script.

# Task I : Lexical-decision experiment data

You will work with a dataset stemming from a lexical-decision experiment. Within this experiment, participants were asked to respond as fast as possible by pressing a `YES` button whenever a real word in German was presented (e.g., das Haus). Whenever the word that was presented was not a real word in 
German (e.g., das Hous), participants were asked to press as fast as possible a `NO` button.

Additional information about the words that were presented in this study can be found in the dataset. For instance, the word length category (“length” variable), or the specific length of a particular word (“word_length” variable). You can also look up the “type” of trial, the frequency level (“freq” variable), the given “response”, the “item” number and the “trial” number.

## Task I.1.

Load in the data `Lexical_Decision_RTs.sav ` and save it in a variable called lexical_decision_dat. Considering that this is a `.sav` file, think about which function is needed. If you need a specific package to load this data, make sure to add the package to the `packages` code chunk above

```{r}
lexical_decision_dat <- haven::read_sav("Lexical_Decision_RTs.sav")
```

## Task I.2.

In this second step, you should get an understanding of the data that you are working with.
Look at the structure of the data, look at what columns you have and what type of data is stored there

```{r}
lexical_decision_dat
```


```{r}
str(lexical_decision_dat)
```

When working with SPSS datafiles such as, .sav files, the structure of the data looks quite differently than it is the case for .csv, or .dat files, or even data frames that can be created with R. However, upo a closer inspection, you should see that there are some labels attributed to some columns: `type`, `freq`, `length`, `response`

## Task I.3.

As you might have already noticed, the columns `type`, `freq`, `length`, `response` contain categorical values, which are now stored as numbers, but thanks to the labels, we know how to make them more informative.

Your next task is to recode the values of each of the categorical variables above. Here is what the values mean for each column. The existing values are on the left side. Please recode the values on the left side with those on the right side

* `type` - column informing about the type of word participants saw

  + 1 = Real word (critical trial)
  + 2 = Pseudo word (filler trial)
  
* `freq` - word frequency

  + 1 = low frequency
  + 2 = medium frequency
  + 3 = high frequency
  + NA = pseudo-words have no word frequency

* `length` - length of the word

  + 1 = short frequency
  + 2 = medium frequency
  + 3 = long frequency

* `response` - participant response. Was the word they saw a word or not?

  + 1 = YES  
  + 2 = N0

Note that there are several ways to do this, feel free to use any option you want.

```{r recoding}
# Updated solution for Task 1.3., 14.07:
# In order to properly be able to recode the NA values within the column of a dataframe, the NAs need to be replaced in two steps. 
# Step 1: Use the replace_na() function to transform the NA value to a value that is the same as the values of the different levels in your column - in our case, we can use 0, since the rest of the levels are 1, 2, and 3
# Step 2: Declare the columns to be recoded as factors, including the column on which you have previously applied the replace_na() function
# Step 3: Use the recode_factor function.


lexical_decision_dat$freq <- lexical_decision_dat$freq %>% 
  replace_na(0)

# Making sure all categorical variables are converted to factors

lexical_decision_dat$type <- as.factor(lexical_decision_dat$type)
lexical_decision_dat$freq <- as.factor(lexical_decision_dat$freq)
lexical_decision_dat$length <- as.factor(lexical_decision_dat$length)
lexical_decision_dat$response <- as.factor(lexical_decision_dat$response)

# It's good practice to save the result of your recoding in a new variable in case you would like to access the original data at a later step
lexical_decision_dat_recoded <- lexical_decision_dat %>% 
  mutate(type = recode_factor(type,
                              "1" = "Real_word",
                              "2" = "Pseudo_word"),
         freq = recode_factor(freq,
                              "1" = "low",
                              "2" = "medium" ,
                              "3" = "high" ,
                              "0" = "Pseudo_word"),
         length = recode_factor(length,
                              "1" = "short",
                              "2" = "medium" ,
                              "3" = "long"  ),
         response = recode_factor(response,
                                  "1" = "YES",
                                  "2" = "NO")
         
         ) 


lexical_decision_dat_recoded

```

## Task I.4. - Data exploration

- **Find out how many participants took part in the experiment**

Do not just type in the answer. Type in the code you used to find out

Hint: Use the functions length() and unique() nested.

```{r}
length(unique(lexical_decision_dat_recoded$participant))
```

- **Find out how many experimental trials were critical and how many were fillers.**

Critical items are those where participants saw real words, whereas within filler trials, participants saw pseudo-words. Do not type just numbers, type the code that helps you find that. You need to find out the number of critical real words trials and filler trials that each participant has seen

Hint: The nrow() function helps you determine the number of rows. If you apply it to a subset of the experimental trials with real words, then you would get the number of trials for all participants. You need to determine the number of trials of each type that each participant has seen

```{r}
# Constructing a subset for the critical items
lexical_decision_dat_recoded_critical <- lexical_decision_dat_recoded %>% 
  filter(type == "Real_word")

# We compute the number of rows associated with the critical trials and then we divide by the number of participants
nrow(lexical_decision_dat_recoded_critical)/32

# Constructing a subset for the filler items
lexical_decision_dat_recoded_filler <- lexical_decision_dat_recoded %>% 
  filter(type == "Pseudo_word")

# We compute the number of rows associated with the critical trials and then we divide by the number of participants
nrow(lexical_decision_dat_recoded_filler)/32
```

There were 54 real word (critical) trials and 54 pseudo-word (filler) trials.

- **Generate frequency tables and plots for each categorical variable: type, frequency, length, response**

Hint: Do you remember the script on outliers? We used a function from a specific package that allowed us to generate both frequency tables and plots for variables. Make sure to add that package to the libraries list at the beginning of the script once you determine which function to choose and from which package it comes. Alternatively you can use the `::` notation

```{r}
epiDisplay::tab1(lexical_decision_dat_recoded$type)
```

```{r}
epiDisplay::tab1(lexical_decision_dat_recoded$freq)
```

```{r}
epiDisplay::tab1(lexical_decision_dat_recoded$length)
```

```{r}
epiDisplay::tab1(lexical_decision_dat_recoded$response)
```

The frequency tables make it easy for us to see what kind of categories we have in our data and how often they occur. They inform you about the design of the experiment and help you see the first patterns in your data

## Task I.5. - Computing accuracy of responses

In this task we will compute participants' response accuracy. In short, their answer needs to match the type of the experiment trial. If they saw a real word, they were supposed to (expected to) press on YES, if they saw a pseudo-word they were supposed to press on NO

- **Create a new column called accuracy and add participant accuracy scores**

Use the code 1 for a correct response and the value 0 for an incorrect response

Hint: Use the `ifelse()` function to help you create the content of the new `accuracy` columm. You need to check for two conditions to be true and depending on this, to set accuracy values of 1 - correct and 0 - incorrect.

Hint 2: Feeling extremely stuck? That's okay! After you've tried several things and feel that you hav exhausted all options, check the answer here to be able to move forward with the script: https://box.hu-berlin.de/f/381bf1ff4e10412d8d69/

```{r}
lexical_decision_dat_recoded <- lexical_decision_dat_recoded %>% 
  mutate(accuracy = ifelse(type == "Real_word" & response == "YES", 1, 0))
```

- **Create a new column called accuracy_percentage based on the accuracy scores**

To do this, you should group data by participants and create an accuracy_percentage column where you should sum up all of the points per participant, divide it by the number of the items and then multiply the result by 100 to obtain the result.

Here comes the tricky part: When you decide to which number you should divide the sum of accuracy scores, think about the fact that you are essentially asking yourself how many times the participants were right in their decisions.

Hint: If this task is a bit too much that's of course okay. Give it your best shot though!
If you have spent too much time on it and you have not found any solution and just want to move on, here's the solution for that:

```{r}
lexical_decision_dat_recoded <- lexical_decision_dat_recoded %>% 
  dplyr::group_by(participant) %>% 
  dplyr::mutate(accuracy_percentage = (sum(accuracy)/54)*100)
```

- **Determine the highest and the lowest accuracy percentages**

Hint: there is a function that gives you both values in one go

```{r}
range(lexical_decision_dat_recoded$accuracy_percentage)
```

- **Determine the average accuracy percentage**

```{r}
mean(lexical_decision_dat_recoded$accuracy_percentage)
```

## Task I.6. Exploring your data visually

We haven't yet learned anything about the `word_length` and `RT` columns - we'll do just that in the next few steps using plots.

- **Plot the distribution of the values within the `word_length` column with a histogram and describe the plot - is the data normally distributed?**

```{r}
hist(lexical_decision_dat_recoded$word_length)
```


- **Plot the distribution of the values within the `RT` column with a histogram and describe the plot - is the data normally distributed?**

```{r}
hist(lexical_decision_dat_recoded$rt)
```

- **Using a boxplot, compare the distribution of the RTs for different word frequencies (“freq”
variable).**

We are now curious to see whether the word length led impacted reading times. As well as that, we are interesed in finding out whether there are outliers in our data. Boxplots are the perfect tool for that (that is, exploring the distribution of a measure variable (rt) as a function of a categorical variable (word length), as well as for detecting outliers)

We should only be working with the real word data, so let's build a subset

```{r}
lexical_decision_dat_critical <- lexical_decision_dat_recoded %>% 
  filter(type =="Real_word")
```

Hint: To achieve this result, the word frequency should be plotted on the x-axis and the reaction times on the y-axis

```{r}
ggplot(data = lexical_decision_dat_critical, aes(x = freq, y = rt, fill = freq))+
  geom_boxplot() +
  stat_boxplot(geom = "errorbar")+
  labs(x = "Word frequency category", y = "Reaction times", fill = "Word frequency category",
       title = "Reaction times as a function of word frequency")+ 
  scale_fill_brewer(palette = "Pastel1")
```

- **Describe what you understand from the boxplot**

The boxplot showing the reaction times plotted as a function of word frequency category informs us with respect to the following patterns:

- the higher the frequency of the word, the less outliers we have. In other words, participants are quicker to respond.

- The less frequent a word is, the more time participants need to respond to the lexical-decision task. In other words, for infrequent words, there were longer reaction times for the decision as to whether the encountered word was a word or not.

- Participants performed much better for high vs. medium frequency words and for medium vs. low frequency words.


# Task II: Festival goers and their hygiene scores

Use the data file (“DownloadFestival.dat”). We can use it to explore assumptions
about our data. The data contains the hygiene scores from three days at the Download Music Festival (the scores from the three days are labeled ‘day1’, ‘day2’, and ‘day3’ respectively). We also recorded participants’s gender and their ticket number.

We plan to analyze the data for day1 using a **parametric t-test**. First,
verify assumptions that your data should meet before conducting the test,
and screen the data for outliers.

## Task II. 1.:  **Read in the data**

Consider the fact that this is a `.dat` file. Which function should you use? Save dataset in a variable called `downloadDat`

```{r}
downloadDat <- haven::read_sav("DownloadFestival_4stud.sav")
downloadDat
```

## Task II.3.: Take a look at the data and get a sense of its structure.

```{r}
str(downloadDat)
```

## Task II.3.: Build a subset with the data for day 1 only, including the information about ticket number and gender. Save the result in a variable called `downloadDat_day1`.

Hint: Use the `select()` function from the `dplyr` package. You can even select out columns, that would be the quickest way. You can also use any other solution that works for you

```{r}
downloadDat_day1 <- downloadDat %>% 
  dplyr::select(-c("day2", "day3"))
```

## Task II.4.: Build subsets for female and male festival goers

This is necessary, because in later steps we will screen each group for outliers and we will test our assumptions for the parametric test. The male gender is coded with 0, the female gender is coded with 1

- **Generate data subsets for each gender**

```{r}
downloadDat_day1_female <- downloadDat_day1 %>% 
  filter(gender == 1)
```

```{r}
downloadDat_day1_male <- downloadDat_day1 %>% 
  filter(gender == 0)
```

## Task II.5.: Screen the female data for outliers

- **Compute z-scores and recode them to rounded values for convenience just like you did in Week2. If you find any scores *larger than 2*, then you should replace these with the average hygiene score. If you find any, replace the hygiene score with the mean hygiene score for the group in which the outlier occurred.**

```{r}
# Saving the mean and the standard deviation in variables
mean_female_day1_download <- mean(downloadDat_day1_female$day1)
sd_female_day1_download <-  sd(downloadDat_day1_female$day1)


downloadDat_day1_female <- downloadDat_day1_female %>% 
  mutate(z_scores = ((downloadDat_day1_female$day1-mean_female_day1_download)/sd_female_day1_download))

# Check whether there are any outliers in the data
downloadDat_day1_female_filtered <- downloadDat_day1_female %>% 
  filter(z_scores > 2)

```


- **Replace outliers with the average hygiene scores for day1 for the female group**

Don't worry, you don't need to complete this task all alone. This is a way to replace values in a column depending on values in another column. 

The code below uses variable names that might differ from yours, so make sure to use the variable name for your female subset and for the variable where you saved the mean hygiene score.

**What does the code below do?**

- It takes the dataset and looks into the `day1` column

- Next, based on the content within the square brackets `[]`, R looks for the row that satisfies the condition within the square brackets

- Next, R replaces the content of that column and row number in the dataset with a new value determined by us.

```{r}
downloadDat_day1_female$day1[downloadDat_day1_female$z_scores > 2] <- mean_female_day1_download
```


## Task II.6.: Screen the male data for outliers

- **Compute z-scores and recode them to rounded values for convenience just like you did in Week2. If you find any scores *larger than 2*, then you should replace these with the average hygiene score. If you find any, replace the hygiene score with the mean hygiene score for the group in which the outlier occurred.**


```{r}
downloadDat_day1_male

mean_male_day1_download <- mean(downloadDat_day1_male$day1)
sd_male_day1_download <-  sd(downloadDat_day1_male$day1)


downloadDat_day1_male <- downloadDat_day1_male %>% 
  mutate(z_scores = ((downloadDat_day1_male$day1-mean_male_day1_download)/sd_male_day1_download))


# Check whether there are any outliers in the data
downloadDat_day1_male_filtered <- downloadDat_day1_male %>% 
  filter(z_scores > 2)
```

- **Replace outliers with the average hygiene scores for day1 for the female group**

```{r}
downloadDat_day1_male$day1[downloadDat_day1_male$z_scores > 2] <- mean_male_day1_download
```


## Task II.7.: What kind of parametric t-test should you be running?

You should run an unpaired t-test, since we are comparing independent groups split by gender

## Task II.8.: Check whether the data fulfills the assumptions

- **Check for the normality assumption - female dataset**

```{r}
nortest::lillie.test(downloadDat_day1_female$day1)
```
- **Check for the normality assumption - male dataset**

```{r}
nortest::lillie.test(downloadDat_day1_male$day1)
```

- **Check for the homogeneity of variance assumption**

Note that you need to merge the female and male subsets into a new dataset. Remember that you have computed z-scores and also removed values, and this is why an updated dataset is needed. As well as that, you need to make sure that gender is coded as a factor

```{r}
downloadDat_day1_new <- rbind(downloadDat_day1_female, downloadDat_day1_male)
```


```{r}
downloadDat_day1_new$gender <- as.factor(downloadDat_day1_new$gender)
```


```{r}
car::leveneTest(day1 ~ gender, data = downloadDat_day1_new)
```

## Task II.9.: Conduct the t-test, report and interpret the results

```{r}
wilcox.test(data =  downloadDat_day1_new,day1 ~ gender, paired = FALSE)
```
This was a trick task. The data did not meet the assumption of homogeneity of variance and thus it is necessary to run a non-parametric test, the Wilcoxon Rank Sum test.

